<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ACT Demystified | Blogs & Insights | Sarah's Portfolio</title>
  <link rel="icon" type="image/x-icon" href="data:image/x-icon;base64," />
  <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin />
  <link rel="stylesheet" href="../css/blogs/gradCAM(summary).css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&family=Noto+Sans:wght@400;500;700&display=swap" />
  <script src="https://cdn.tailwindcss.com?plugins=forms,container-queries"></script>
</head>
<body>
  <div class="layout-container flex flex-col min-h-screen bg-white">
    <!-- Header -->
    <header class="header">
      <div class="header-left">
        <a href="../index.html" class="logo-link" aria-label="Home">
          <svg class="logo-icon" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
            <g clip-path="url(#clip0_6_319)">
              <path
                d="M8.57829 8.57829C5.52816 11.6284 3.451 15.5145 2.60947 19.7452C1.76794 23.9758 2.19984 28.361 3.85056 32.3462C5.50128 36.3314 8.29667 39.7376 11.8832 42.134C15.4698 44.5305 19.6865 45.8096 24 45.8096C28.3135 45.8096 32.5302 44.5305 36.1168 42.134C39.7033 39.7375 42.4987 36.3314 44.1494 32.3462C45.8002 28.361 46.2321 23.9758 45.3905 19.7452C44.549 15.5145 42.4718 11.6284 39.4217 8.57829L24 24L8.57829 8.57829Z"
                fill="currentColor"
              ></path>
            </g>
            <defs>
              <clipPath id="clip0_6_319"><rect width="48" height="48" fill="white"></rect></clipPath>
            </defs>
          </svg>
          <span class="site-title">Portfolio</span>
        </a>
      </div>
      <nav class="header-nav" aria-label="Main navigation">
        <a href="../projects.html">Projects</a>
        <a href="../resume.html">Resume/About Me</a>
        <a href="../achievements.html">Achievements</a>
        <a href="../blogs.html" class="active">Blogs/Insights</a>
        <a href="../contact.html">Contact</a>
      </nav>
      <div class="header-actions">
        <button class="mode-toggle" aria-label="Toggle light/dark mode">
          <svg width="20" height="20" fill="currentColor" viewBox="0 0 256 256">
            <path d="M120,40V16a8,8,0,0,1,16,0V40a8,8,0,0,1-16,0Zm72,88a64,64,0,1,1-64-64A64.07,64.07,0,0,1,192,128Zm-16,0a48,48,0,1,0-48,48A48.05,48.05,0,0,0,176,128ZM58.34,69.66A8,8,0,0,0,69.66,58.34l-16-16A8,8,0,0,0,42.34,53.66Zm0,116.68-16,16a8,8,0,0,0,11.32,11.32l16-16a8,8,0,0,0-11.32-11.32ZM192,72a8,8,0,0,0,5.66-2.34l16-16a8,8,0,0,0-11.32-11.32l-16,16A8,8,0,0,0,192,72Zm5.66,114.34a8,8,0,0,0-11.32,11.32l16,16a8,8,0,0,0,11.32-11.32ZM48,128a8,8,0,0,0-8-8H16a8,8,0,0,0,0,16H40A8,8,0,0,0,48,128Zm80,80a8,8,0,0,0-8,8v24a8,8,0,0,0,16,0V216A8,8,0,0,0,128,208Zm112-88H216a8,8,0,0,0,0,16h24a8,8,0,0,0,0-16Z"></path>
          </svg>
        </button>
        <div class="profile-pic" style='background-image: url("../assets/profile.jpg");' aria-label="Profile picture"></div>
      </div>
    </header>

    <!-- Main Content -->
    <main class="main-content">
      <nav class="breadcrumb">
        <a href="../blogs.html">Insights</a>
        <span>/</span>
        <span class="current">ACT Demystified</span>
      </nav>
      <article class="blog-article">
        <h1 class="blog-title">ACT Demystified: Action Chunking with Transformers for Robotic Manipulation</h1>
        <p class="blog-date">Published on July 6, 2025</p>

        <section class="blog-section">
          <h2>Paper Overview</h2>
          <p>
            <strong>Paper Title:</strong> Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (ACT: Action Chunking with Transformers)
          </p>
          <p>
            ACT is a novel imitation learning algorithm that enables robots to perform complex, precise manipulation tasks using only a small number of human demonstrations. It was developed to address the challenges of fine-grained bimanual manipulation, such as threading a zip tie or opening a condiment cup, using affordable hardware and robust learning techniques[13].
          </p>
        </section>

        <section class="blog-section">
          <h2>Why ACT? The Challenge</h2>
          <ul>
            <li><strong>Compounding Errors:</strong> Small mistakes in predicted actions can accumulate, causing the robot to fail at tasks that require precision.</li>
            <li><strong>Non-Markovian Human Behavior:</strong> Human demonstrations often include pauses or correlated actions that are hard for simple policies to model.</li>
            <li><strong>Noisy Demonstrations:</strong> Human data is variable—even for the same task, the way it’s done can change each time.</li>
          </ul>
          <p>
            Traditional imitation learning methods struggle with these issues, especially for tasks that need high-frequency, closed-loop control and visual feedback[13].
          </p>
        </section>

        <section class="blog-section">
          <h2>Core Innovations of ACT</h2>
          <ul>
            <li><strong>Action Chunking:</strong> Instead of predicting one action at a time, ACT predicts a sequence (chunk) of actions for the next <em>k</em> steps. This reduces the effective task horizon and helps prevent errors from compounding.</li>
            <li><strong>Temporal Ensembling:</strong> To keep robot motion smooth, ACT averages overlapping action predictions at each timestep, using exponential weights to prioritize recent observations.</li>
            <li><strong>Conditional Variational Autoencoder (CVAE):</strong> ACT models the variability in human demonstrations by learning a distribution over action sequences, making it robust to noisy data.</li>
            <li><strong>Transformer Architecture:</strong> ACT uses transformers to model long-range dependencies in both observations and actions, enabling it to learn complex, coordinated behaviors.</li>
          </ul>
        </section>

        <section class="blog-section">
          <h2>How ACT Works: Step-by-Step</h2>
          <ol>
            <li>
              <strong>Action Chunking:</strong> Given the current observation (images and joint positions), ACT predicts the next <em>k</em> actions (target joint positions for both arms). This chunking reduces the number of decisions the policy must make, making learning easier and more stable.
            </li>
            <li>
              <strong>Temporal Ensembling:</strong> At every timestep, ACT combines multiple predictions for the same action (from overlapping chunks) using a weighted average. This smooths out the robot’s movements and reduces the impact of prediction errors.
            </li>
            <li>
              <strong>CVAE Training:</strong> During training, ACT uses a CVAE to encode the variability in human demonstrations. The encoder compresses action sequences and joint data into a latent variable, while the decoder (policy) predicts action sequences conditioned on this variable and the current observation.
            </li>
            <li>
              <strong>Transformer Backbone:</strong> Both the encoder and decoder use transformer layers to process sequences, allowing the model to capture complex temporal dependencies and coordinate multi-step actions.
            </li>
          </ol>
        </section>

        <section class="blog-section">
          <h2>Implementation Details</h2>
          <ul>
            <li>Observations include four RGB images (from different camera angles) and joint positions for two robot arms.</li>
            <li>Actions are the target joint positions for both arms over the next <em>k</em> timesteps.</li>
            <li>Images are processed with ResNet18 backbones, and features are combined with joint data and the latent variable before being passed to the transformer.</li>
            <li>Typical chunk size: <strong>k = 100</strong> (i.e., predict 2 seconds of actions at 50Hz).</li>
            <li>Model size: ~80 million parameters; training takes about 5 hours on a single GPU.</li>
          </ul>
        </section>

        <section class="blog-section">
          <h2>Experimental Results</h2>
          <ul>
            <li>ACT was tested on 6 real-world and 2 simulated bimanual manipulation tasks, such as opening a cup, threading velcro, and inserting batteries.</li>
            <li>Each task used only 10–20 minutes of human demonstration data (50–100 episodes).</li>
            <li><strong>Success rates:</strong> ACT achieved 80–96% on most real-world tasks, outperforming previous methods by a large margin.</li>
            <li>On challenging tasks like Thread Velcro, perception limitations (e.g., low contrast) reduced success, highlighting areas for future improvement.</li>
          </ul>
        </section>

        <section class="blog-section">
          <h2>Key Techniques: At a Glance</h2>
          <table>
            <thead>
              <tr>
                <th>Technique</th>
                <th>Purpose</th>
                <th>Benefit</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Action Chunking</td>
                <td>Predicts a sequence of actions at once</td>
                <td>Reduces compounding errors, models complex behaviors</td>
              </tr>
              <tr>
                <td>Temporal Ensembling</td>
                <td>Combines overlapping predictions</td>
                <td>Smoother, more accurate actions</td>
              </tr>
              <tr>
                <td>CVAE</td>
                <td>Models demonstration variability</td>
                <td>Handles noisy, non-deterministic data</td>
              </tr>
              <tr>
                <td>Transformers</td>
                <td>Sequence modeling</td>
                <td>Captures long-range dependencies</td>
              </tr>
            </tbody>
          </table>
        </section>

        <section class="blog-section">
          <h2>Limitations & Future Directions</h2>
          <ul>
            <li>ACT struggles with tasks that require very fine perception (e.g., unwrapping candies, opening flat ziploc bags) due to ambiguous visual cues or deformable objects.</li>
            <li>Performance can be improved with more demonstration data, better perception models, or pretraining.</li>
          </ul>
        </section>

        <section class="blog-section">
          <h2>Takeaways</h2>
          <ul>
            <li>ACT enables low-cost robots to perform complex, precise tasks with minimal human demonstrations.</li>
            <li>Combining action chunking, temporal ensembling, and transformer-based sequence modeling is key to its success.</li>
            <li>ACT is a step forward for practical, accessible robot learning in real-world settings.</li>
          </ul>
        </section>

        <div class="back-link">
          <a href="../blogs.html">&larr; Back to Insights</a>
        </div>
      </article>
    </main>
  </div>
</body>
</html>
